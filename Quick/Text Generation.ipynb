{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:dodgerblue\">Text Generation in Bedrock</p>\n",
    "<hr style=\"border:1px dotted; color:floralwhite\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements for this lab\n",
    "*See <span style=\"color:gold\">Appendix</span> at the bottom of this lab to install requirements for this lab*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px dotted\">\n",
    "<hr style=\"border:1px dotted;color:cadetblue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:cadetblue\">Import libraries and set clients</p>\n",
    "*If the following does not show the latest (installed or upgraded) boto3 (at least v1.29.6), restart the kernel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.33.9'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3, json\n",
    "\n",
    "# region_name is important especially if you auth via aws configure and it is set to a region where there is no Bedrock yet\n",
    "#define common vars\n",
    "myRegion='us-west-2'\n",
    "\n",
    "# get clients\n",
    "bedrockRun = boto3.client(service_name='bedrock-runtime', region_name=myRegion)\n",
    "\n",
    "boto3.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px dotted;color:cadetblue\">\n",
    "<hr style=\"border:1px dotted;color:crimson\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:crimson\">Create some methods to call</p>\n",
    "1. Method for processing a prompt\n",
    "2. Method to have a conversation\n",
    "3. <b>Don't forget if you modify this method, make sure you execute the cell</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"color:crimson\">This method processes a single prompt with the chosen foundation model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes a single prompt\n",
    "def processPrompt(m, prompt, p, t, k, n):\n",
    "    # prep some vars\n",
    "    body=''\n",
    "    outputResponse='Model: {}\\n---------------------------------------\\n'.format(m)\n",
    "    accept='application/json'\n",
    "    contentType='application/json'\n",
    "    tk=2000\n",
    "\n",
    "    # which model are we calling\n",
    "    match m:\n",
    "        case 'ai21.j2-ultra-v1' | 'ai21.j2-mid-v1': \n",
    "            # AI21 Labs - Jurassic-2 Ultra and Mid \n",
    "            # https://www.ai21.com/studio\n",
    "            body = json.dumps({\n",
    "                \"prompt\": prompt,\n",
    "                \"temperature\": t, \n",
    "                \"topP\": p, \n",
    "                \"maxTokens\": tk,\n",
    "                \"numResults\": n,\n",
    "            })\n",
    "        case 'amazon.titan-text-express-v1':\n",
    "            # Amazon - Titan\n",
    "            # https://aws.amazon.com/bedrock/titan/\n",
    "            body = json.dumps({\n",
    "                \"inputText\": prompt,\n",
    "                \"textGenerationConfig\": {\n",
    "                    \"temperature\": t,\n",
    "                    \"topP\": p,\n",
    "                    \"maxTokenCount\": tk,\n",
    "                },\n",
    "            })\n",
    "        case 'cohere.command-text-v14':\n",
    "            # Cohere - Command\n",
    "            # https://cohere.com/\n",
    "            body = json.dumps({\n",
    "                \"prompt\": prompt,\n",
    "                \"temperature\": t,\n",
    "                \"p\": p,\n",
    "                \"k\": k,\n",
    "                \"max_tokens\": tk,\n",
    "                \"num_generations\": n,\n",
    "            })\n",
    "        case 'mistral.mistral-7b-instruct-v0:2':\n",
    "            # Mistral - Mistral\n",
    "            # https://mistral.ai/\n",
    "            body = json.dumps({\n",
    "                \"prompt\": prompt,\n",
    "                \"temperature\": t,\n",
    "                \"top_p\": p,\n",
    "                \"top_k\": k,\n",
    "                \"max_tokens\": tk,\n",
    "            })\n",
    "        case 'meta.llama2-13b-chat-v1':\n",
    "            # Meta - Llama 2\n",
    "            # https://llama.meta.com/\n",
    "            body = json.dumps({\n",
    "                \"prompt\": prompt,\n",
    "                \"temperature\": t,\n",
    "                \"top_p\": p,\n",
    "                \"max_gen_len\": tk,\n",
    "            })\n",
    "        case 'anthropic.claude-v2:1':\n",
    "            # Anthropic - Claude\n",
    "            # https://www.anthropic.com/\n",
    "            body=json.dumps({\n",
    "                \"prompt\": \"\\n\\nHuman:{}\\n\\nAssistant:\".format(prompt),\n",
    "                \"temperature\": t,\n",
    "                \"top_p\": p,\n",
    "                \"top_k\": k,\n",
    "                \"max_tokens_to_sample\": tk,\n",
    "            })  \n",
    "\n",
    "    #invoke the model\n",
    "    response = bedrockRun.invoke_model(body=body, modelId=m, accept=accept, contentType=contentType)\n",
    "    responseBody = json.loads(response.get('body').read())\n",
    "\n",
    "    # get the response\n",
    "    match m:\n",
    "        case 'ai21.j2-ultra-v1' | 'ai21.j2-mid-v1':            \n",
    "            for response in range(n):\n",
    "                outputText = responseBody.get('completions')[response].get('data').get('text')\n",
    "                outputResponse += 'RESPONSE: {}\\n{}\\n---------------------------------------\\n'.format(response+1, outputText)\n",
    "        case 'amazon.titan-text-express-v1':\n",
    "            outputText = responseBody['results'][0]['outputText']\n",
    "            outputResponse += '1 RESPONSE Supported:\\n{}\\n---------------------------------------\\n'.format(outputText)\n",
    "        case 'cohere.command-text-v14':\n",
    "            for response in range(n):\n",
    "                outputText = responseBody['generations'][response]['text']\n",
    "                outputResponse += 'RESPONSE: {}\\n{}\\n---------------------------------------\\n'.format(response+1, outputText)\n",
    "        case 'mistral.mistral-7b-instruct-v0:2':\n",
    "            outputText = responseBody['outputs'][0]['text']\n",
    "            outputResponse += '1 RESPONSE Supported:\\n{}\\n---------------------------------------\\n'.format(outputText)\n",
    "        case 'meta.llama2-13b-chat-v1':\n",
    "            outputText = responseBody['generation']\n",
    "            outputResponse += '1 RESPONSE Supported:\\n{}\\n---------------------------------------\\n'.format(outputText)\n",
    "        case 'anthropic.claude-v2:1':\n",
    "            outputText = responseBody['completion']\n",
    "            outputResponse += '1 RESPONSE Supported:\\n{}\\n---------------------------------------\\n'.format(outputText)\n",
    "\n",
    "    # return response\n",
    "    return outputResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"color:crimson\">This following methods create a chatbot with the chosen foundation model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process chatbot conversation\n",
    "# langchain required as models are stateless\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "def chatBot(m, p, t): # demo_chatbot\n",
    "    # prep some vars\n",
    "    tk = 1000\n",
    "\n",
    "    # following connections the FM to langchain\n",
    "    # dont need a prompt yet as we haven't started the conversation\n",
    "    match m:\n",
    "        case \"ai21.j2-ultra-v1\" | \"ai21.j2-mid-v1\":\n",
    "            # AI21 Labs - Jurassic-2 Ultra and Mid \n",
    "            # https://www.ai21.com/studio\n",
    "            cl_llm = Bedrock(\n",
    "                model_id=m,\n",
    "                client=bedrockRun,\n",
    "                model_kwargs={\n",
    "                    \"temperature\": t, \n",
    "                    \"topP\": p,\n",
    "                    \"maxTokens\": tk,\n",
    "                },\n",
    "            )\n",
    "        case 'amazon.titan-text-express-v1':\n",
    "            # AI21 Labs - Jurassic-2 Ultra and Mid \n",
    "            # https://www.ai21.com/studio\n",
    "            cl_llm = Bedrock(\n",
    "                model_id=m,\n",
    "                client=bedrockRun,\n",
    "                model_kwargs={\n",
    "                    \"temperature\": t,\n",
    "                    \"topP\": p,\n",
    "                    \"maxTokenCount\": tk,\n",
    "                },\n",
    "            )\n",
    "        case \"meta.llama2-13b-chat-v1\" | \"meta.llama2-70b-chat-v1\":\n",
    "            # Meta - Llama 2\n",
    "            # https://llama.meta.com/\n",
    "            cl_llm = Bedrock(\n",
    "                model_id=m,\n",
    "                client=bedrockRun,\n",
    "                model_kwargs={\n",
    "                    \"temperature\": t,\n",
    "                    \"top_p\": p,\n",
    "                    \"max_gen_len\": tk,\n",
    "                },\n",
    "            )\n",
    "        case 'anthropic.claude-v2:1':\n",
    "            # Anthropic - Claude\n",
    "            # https://www.anthropic.com/\n",
    "            cl_llm = Bedrock(\n",
    "                model_id=m,\n",
    "                client=bedrockRun,\n",
    "                model_kwargs={\n",
    "                    \"temperature\": t,\n",
    "                    \"top_p\": p,\n",
    "                    \"max_tokens_to_sample\": tk,\n",
    "                    \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "                },\n",
    "            )\n",
    "\n",
    "    return cl_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatbot memory to remember conversations\n",
    "def chatBotMemory(m, p, t): # demo_memory\n",
    "    llmData=chatBot(m, p, t)\n",
    "    memory = ConversationBufferMemory(llm=llmData, max_token_limit=5000)\n",
    "    return memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatbot conversation chain\n",
    "def chatBotChain(prompt, m, p, t, memory): # demo_conversation\n",
    "    llmChain=chatBot(m, p, t)\n",
    "    llmConversation=ConversationChain(llm=llmChain, memory=memory, verbose=True)\n",
    "\n",
    "    response=llmConversation.predict(input=prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process prompt suitable for the model\n",
    "def chatBotPrompt(prompt, m, useTemplate):\n",
    "    # provide a template to stop model from simulation Human replies and continuing the conversation on its own\n",
    "    # you can tell the model how to answer, eg you are a primary school teacher, or answer in French, etc\n",
    "    if useTemplate:\n",
    "        t='{who1}[INST]You are a helpful bot which answers one question at a time.[/INST]{who2}\\n{who1}{human}{who2}'\n",
    "    else:\n",
    "        t='{who1}{human}{who2}'\n",
    "\n",
    "    match m:\n",
    "        case \"ai21.j2-ultra-v1\" | \"ai21.j2-mid-v1\":\n",
    "            # AI21 Labs - Jurassic-2 Ultra and Mid \n",
    "            # https://www.ai21.com/studio\n",
    "            prompt=t.format(who1='', human=prompt, who2='')\n",
    "        case 'amazon.titan-text-express-v1':\n",
    "            # AI21 Labs - Jurassic-2 Ultra and Mid \n",
    "            # https://www.ai21.com/studio\n",
    "            prompt=t.format(who1='User: ', human=prompt, who2='\\nBot:')\n",
    "        case \"meta.llama2-13b-chat-v1\" | \"meta.llama2-70b-chat-v1\":\n",
    "            # Meta - Llama 2\n",
    "            # https://llama.meta.com/\n",
    "            prompt=t.format(who1='', human=prompt, who2='')\n",
    "        case 'anthropic.claude-v2:1':\n",
    "            # Anthropic - Claude\n",
    "            # https://www.anthropic.com/\n",
    "            prompt=t.format(who1='\\n\\nHuman: ', human=prompt, who2='\\n\\nAssistant:')\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:crimson\">Now we can start using our models with prompts</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px dotted;color:crimson\">\n",
    "<hr style=\"border:1px dotted;color:lightblue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:lightblue\">Invoke a model with a single prompt</p>\n",
    "1. Specify properties\n",
    "2. Call the method\n",
    "3. Prompt engineering <a href='https://docs.aws.amazon.com/bedrock/latest/userguide/general-guidelines-for-bedrock-users.html'>help</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the prompt and params - note each model will have different ranges for its params\n",
    "prompt='You are a primary school teacher teaching 10 year old children. Write a class presentation about our solar system. Answer in Thai.'\n",
    "t=0.7 # temperature: use a lower value to decrease randomness in the response\n",
    "p=0.5 # topP: use a lower value to ignore less probable options\n",
    "k=50 # topK: number of most-likely candidates that the model considers for the next token\n",
    "n=2 # numResults: how many responses do you want, not all models support multiple responses\n",
    "\n",
    "# which model do you want to use based on the match statement below\n",
    "u=7\n",
    "\n",
    "match u:\n",
    "    case -1: # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "        # AI21 Labs - Jurassic-2 Ultra\n",
    "        # property ranges: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-jurassic2.html\n",
    "        m='ai21.j2-ultra-v1'\n",
    "    case -2: # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "        # AI21 Labs - Jurassic-2 Mid\n",
    "        # property ranges: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-jurassic2.html\n",
    "        m='ai21.j2-mid-v1'\n",
    "    case 3:\n",
    "        # Amazon - Titan\n",
    "        # property ranges: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-text.html\n",
    "        m='amazon.titan-text-express-v1'\n",
    "    case -4: # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "        # Cohere - Command\n",
    "        # property ranges: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-command.html\n",
    "        m='cohere.command-text-v14'\n",
    "    case 5:\n",
    "        # Mistral - Mistral\n",
    "        # property ranges: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-mistral.html\n",
    "        m='mistral.mistral-7b-instruct-v0:2'\n",
    "    case -6: # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "        # Meta - Llama 2\n",
    "        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html\n",
    "        m='meta.llama2-13b-chat-v1'\n",
    "    case 7:\n",
    "        # Anthropic - Claude\n",
    "        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-text-completion.html\n",
    "        m='anthropic.claude-v2:1'\n",
    "\n",
    "o=processPrompt(m, prompt, p, t, k, n)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px dotted;color:lightblue\">\n",
    "<hr style=\"border:1px dotted;color:deeppink\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:deeppink\">Invoke a chatbot model with a conversational prompt</p>\n",
    "1. Specify properties\n",
    "2. Call the method\n",
    "3. Prompt engineering <a href='https://docs.aws.amazon.com/bedrock/latest/userguide/general-guidelines-for-bedrock-users.html'>help</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the prompt and params - note each model will have different ranges for its params\n",
    "prompt='hello'\n",
    "t=0 # temperature: use a lower value to decrease randomness in the response\n",
    "p=0.1 # topP: use a lower value to ignore less probable options\n",
    "k=50 # topK: number of most-likely candidates that the model considers for the next token\n",
    "\n",
    "# which model do you want to use based on the match statement below\n",
    "u=3\n",
    "\n",
    "match u:\n",
    "    case -1: # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "        # AI21 Labs - Jurassic-2 Ultra\n",
    "        # property ranges: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-jurassic2.html\n",
    "        m='ai21.j2-ultra-v1'\n",
    "    case -2: # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "        # AI21 Labs - Jurassic-2 Mid\n",
    "        # property ranges: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-jurassic2.html\n",
    "        m='ai21.j2-mid-v1'\n",
    "    case 3:\n",
    "        # Amazon - Titan\n",
    "        # property ranges: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-text.html\n",
    "        m='amazon.titan-text-express-v1'\n",
    "    case -4: # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "        # Meta - Llama 2\n",
    "        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html\n",
    "        m='meta.llama2-70b-chat-v1'\n",
    "    case 5:\n",
    "        # Anthropic - Claude\n",
    "        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-text-completion.html\n",
    "        m='anthropic.claude-v2:1'\n",
    "\n",
    "# format prompt suitable for conversation\n",
    "prompt=chatBotPrompt(prompt, m, True)\n",
    "print(\"YOUR PROMPT FORMATTED:\\n\" + prompt)\n",
    "# start chatbot\n",
    "memory=chatBotMemory(m, p, t)\n",
    "o=chatBotChain(prompt, m, p, t, memory)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue the conversation\n",
    "prompt=chatBotPrompt(\"How old is planet earth?\", m, False)\n",
    "o=chatBotChain(prompt, m, p, t, memory)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue the conversation\n",
    "prompt=chatBotPrompt(\"How do you know that?\", m, False)\n",
    "o=chatBotChain(prompt, m, p, t, memory)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px dotted;color:crimson\">\n",
    "<hr style=\"border:1px dotted;color:lightgreen\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:lightskyblue\">Anthropic - Claude - Using the Claude Massage API - Latest Gen</p>\n",
    "1. Set up Bedrock request\n",
    "2. Set the controls (https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html)\n",
    "3. Invoke the model with the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "def claudeMessageAPI(bedrock_runtime, model_id, system_prompt, messages, max_tokens):\n",
    "\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"system\": system_prompt,\n",
    "            \"messages\": messages\n",
    "        }  \n",
    "    )  \n",
    "\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "   \n",
    "    return response_body\n",
    "\n",
    "# https://docs.anthropic.com/claude/reference/claude-on-amazon-bedrock\n",
    "modelIdC = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "\n",
    "# Prompt with user turn only.\n",
    "user_message =  {\"role\": \"user\", \"content\": \"You are the anthropic.claude-3-haiku-20240307-v1 foundation model. Describe yourself in the first person using a maximum of 200 words on the following topics:  1. What you have been trained on. 2. Your purpose. 3. Your proudest achievement. 4. Provide detail of up to 5 parameters of yourself listing their numeric values if appropriate. 6. Tell us what you hope for your future.\"}\n",
    "messages = [user_message]\n",
    "systemPromptC = \"Please respond in Thai.\"\n",
    "\n",
    "response = claudeMessageAPI (bedrockRun, modelIdC, systemPromptC, messages, 2000)\n",
    "\n",
    "print(json.dumps(response, indent=4))\n",
    "text_value = response['content'][0]['text']\n",
    "print(text_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px dotted;color:coral\">\n",
    "<hr style=\"border:1px dotted;color:gold\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:gold\">Appendix - Install Requirements (macOS)</p>\n",
    "# Requirements for this lab\n",
    "\n",
    "#### <p style=\"color:deeppink\">- If you are running VSCode on a laptop, follow all of below.<br>- If you are running Jupyter inside an AWS Account, you just need to install langchain. See 2.1 below and skip everything else.</p>\n",
    "\n",
    "*Windows requirements will be similar, apart from Homebrew.*  \n",
    "* python must be installed on your client, and be at least v3.8\n",
    "  * 3.8 supports the latest release of boto3 which supports Bedrock  \n",
    "* boto3 must be installed on your client, and be at least v1.33.9  \n",
    "  * *Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, which allows Python developers to write software that makes use of services like Amazon S3 and Amazon EC2.*\n",
    "  * https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n",
    "### <p style=\"color:gold\">1. Homebrew</p> \n",
    "If you haven't installed Homebrew, you can install it by running the following command here or in the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"color:gold\">1. Python</p> \n",
    "Once Homebrew is installed, you can install Python using the following command  \n",
    "*check what you have before installing/upgrading*  \n",
    "*you will need to quit and restart vsCode to use python once installed (or updated)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.2\n",
      "/opt/homebrew/bin/python3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 --version\n",
    "which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "brew install python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"color:gold\">2. boto3 and other Python requirements</p> \n",
    "*check what you have before installing/upgrading*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: boto3\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'python3 -m pip show boto3\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython3 -m pip show boto3\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py:2493\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2491\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2492\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2493\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2495\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2496\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'python3 -m pip show boto3\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 -m pip show boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -U boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"color:gold\">2.1 Chatbot requirements</p> \n",
    "*<p style=\"color:deeppink\">If you are running a Jupyter in AWS Account, you ONLY need these 2 cells</p>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"color:gold\">3. aws configure</p> \n",
    "*Configure aws configure with credentials, and a user that has all of the Bedrock IAM policies required*  \n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/security_iam_id-based-policy-examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws sts get-caller-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"color:gold\">4. Request Bedrock model access</p> \n",
    "*You must request access to the models required, you may need to provide use case details before you are able to request*  \n",
    "*Make sure you request in the region you intend to use the models in, this lab is us-west-2*  \n",
    "https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/modelaccess  \n",
    "\n",
    "Models required in this lab:\n",
    "\n",
    "* See code above for use of models and what access to request\n",
    "\n",
    "#### Pricing\n",
    "*Use 6 characters per token as an approximation for the number of tokens.*  \n",
    "https://aws.amazon.com/bedrock/pricing/  \n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-prepare.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px dotted;color:gold\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
